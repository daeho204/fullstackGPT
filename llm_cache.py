import time
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.globals import set_llm_cache
from langchain_core.caches import InMemoryCache
from langchain_community.cache import SQLiteCache
from langchain.callbacks import StreamingStdOutCallbackHandler

load_dotenv()

# InMemoryCacheëŠ” ì¬ì‹¤í–‰ ì‹œ ê¸°ë¡ì´ ì§€ì›Œì ¸ì„œ ë‹¤ì‹œ ì§ˆë¬¸í•˜ëŠ” ê³¼ì •ì„ ê±°ì³ì•¼í•œë‹¤
# set_llm_cache(InMemoryCache())

# sqliteë¥¼ ì‚¬ìš©í•œ ìºì‹±
set_llm_cache(SQLiteCache("cache.db"))

model = ChatOpenAI(
    temperature=0.1
)
start_time = time.time()
response_1  = model.predict("explain why tesla stock is rapidly changing")
end_time = time.time()
first_d = end_time - start_time
print(f"â±ï¸ ì²« ë²ˆì§¸ ì‹¤í–‰ (ìºì‹± ì „): {first_d:.4f}ì´ˆ")
print(f"ğŸ“ ì‘ë‹µ ë‚´ìš©: {response_1}\n")

# ğŸ“Œ 2ï¸âƒ£ ìºì‹± í›„ ë‘ ë²ˆì§¸ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
start_time = time.time()
response_2 = model.predict("explain why tesla stock is rapidly changing")
end_time = time.time()
second_run_duration = end_time - start_time
print(f"âš¡ ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹± í›„): {second_run_duration:.4f}ì´ˆ")
print(f"ğŸ“ ì‘ë‹µ ë‚´ìš©: {response_2}\n")
reduction = first_d - second_run_duration
percentage = (reduction / first_d) * 100 if first_d > 0 else 0
print(f"ğŸš€ ìºì‹±ìœ¼ë¡œ ì¸í•œ ì†ë„ í–¥ìƒ: {reduction:.4f}ì´ˆ (ì•½ {percentage:.2f}%)")